---
layout: post
title: MINE으로 데이터 상관관계 분석하기  
permalink: mine
---
{% include mathjax.html %}


## MIC: Maximal Information Coefficient
***

데이터 분석을 하는데 가장 첫 번째 단계는 데이터의 이해이다. 데이터의 특성을 이해하고 분석하는 것 그것이 의미 있는 분석을 하기 위한 첫 걸음이다. 이 단계에서 하는 것이 흔히 feature 사이의 관계를 분석하여 중요한 feature를 찾고, 다른 feature에 의존성이 많은 feature는 제거하는 과정을 거친다.

관계성(Correlation)은 어떻게 찾을 수 있을까?

다양한 correlation metric이 이미 알려져 있다. 흔히 Pearson correlation, Spearman's rank correlation, Mutual information 등이 널리 쓰이는 metric이다. 두 개의 random variables 사이의 dependency를 적절하게 표현하기 위한 노력이 지속적으로 있었으며, 그 중 하나인 MIC(Maximal Information Coefficient)에 대해 소개해보고자 한다. 

## 들어가며...
***

MIC는 David N. Reshef + 지도교수 등이 2011 Science Journal에 소개한 개념이다.
2017년 1월 현재 citation 700이 넘었다.
Basic concept은 이름에서 알 수 있듯 mutual information에서 가져왔다.
잠시 mutual information $$I(X;Y)$$에 대해 먼저 살펴보자.
($$X$$와 $$Y$$는 random variable이다.)

$$
\begin{align*}
    I(X;Y) 
    & := \mathbb{E}\left[\log \left( \frac{p(X,Y)}{p(X) p(Y)} \right) \right] \\
    & \ = \mathbb{E}\left[\log \left( \frac{p(X|Y) p(Y)}{p(X) p(Y)} \right) \right] \\
    & \ = \mathbb{E}\left[\log \left( \frac{p(X | Y)}{p(X)} \right) \right] \\
    & \ = \mathbb{E}\left[\log \left( p(X | Y) - \log p(X) \right) \right]
\end{align*}
$$

마지막 식에서 $$X$$와 $$Y$$가 독립이라면 $$I(X;Y)=0$$이며 $$Y$$로부터 $$X$$에 대해 알 수 있는 정보량이 없다는 것을 의미한다.
실제로 $$I(X;Y) = 0$$ if and only if $$X$$ and $$Y$$ are independent가 성립한다.
또한, Entropy $$H(X)=\mathbb{E}[-log(X)]$$를 이용해 위의 식을 좀 더 정리하면 아래와 같은 식을 얻는다.

\begin{align}
    I(X;Y) = H(X) - H(X|Y)
\end{align}

$$H(X)$$가 randomness를 측정하는 지표이므로 ($$X$$가 상수이면 $$H(X)=0$$이다.) $$I(X;Y)$$는 $$Y$$에 의해 얻어진 정보량을 나타낸다고 볼 수 있다.


## Intuition
***

Distribution을 알고 있다면 dependency는 식으로 계산하면 되지만, 데이터 분석을 하는 동안에는 가진거라고는 오로지 sample data 뿐이다. 여기서 어떻게 dependency를 계산하면 좋을까. 먼저 두 개의 feature X, Y에 대해서 Scatter plot을 그리고 생각하자. 

![No image](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3325791/bin/nihms358982f1.jpg "MIC Computation Step refered in [1]"){:.center-img}

위 그림은 주어진 data sample에서 어떻게 관계를 찾는지를 직관적으로 설명해준다.
둘 사이에 correlation이 크다면 scatter set $$S=\{(X,Y)\}$$가 특정한 모양으로 나타날 것이다.
$$S$$를 적당히 grid로 잘라 그 correlation function을 나타내는 영역을 반드시 찾을 수 있다.
그림에서는 concave function을 나타내고 있지만 linear 함수든 sinusoidal이든 뭐든 상관없다.
Noiseless data라면 적당한 함수가 있어 그에 따라 scatter plot이 그려진다.
즉, scatter plot은 relation function을 나타내며 grid로 잘라 각 영역의 mutual information을 계산하여 relationship을 추정하는 것이 가능해보인다.
이것이 MIC의 intuition이다.


## Definition and Properties
***

이제 앞서 살펴 본 직관을 바탕으로 정의를 살펴보자. 

blahblah

[Prop 1]

[Prop 2]

[Prop 3]

# References
***

1. D. N. Reshef, Y. A. Reshef, H. K. Finucane, S. R. Grossman, G. McVean, P. J. Turnbaugh, E. S. Lander, M. Mitzenmacher, P. C. Sabeti, "Detecting Novel Associations in Large Data Sets," Science, 334 (6062), pp. 1518–1524, 2011.
1. https://www.r-bloggers.com/maximal-information-coefficient-mic/  
1. https://www.r-bloggers.com/maximal-information-coefficient-part-ii/  
1. http://menugget.blogspot.kr/2011/12/maximal-information-coefficient-mic.html#more  